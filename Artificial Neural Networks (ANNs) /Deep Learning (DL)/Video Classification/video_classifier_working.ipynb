{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b80bf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64a42a09",
   "metadata": {},
   "source": [
    "<h1><center>Video Classifier Using Deep Learning</center></h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3bb6dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26eeb563",
   "metadata": {},
   "source": [
    "<h3>Problem Statement</h3>\n",
    "\n",
    "Video Classification is implemented using CNN and RNN architecture.<br>\n",
    "Training & Testing Dataset are downloaded from pixabay.com & stored locally to be used in this project.<br>\n",
    "Due to space constraint, only few videos (of category car, forest & space) are downloaded & used in this project.<br>\n",
    "As a result, accuracy of this project is not very good. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4485a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b4b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\0394D9744\\Anaconda3\\envs\\mtech\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\0394D9744\\Anaconda3\\envs\\mtech\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\0394D9744\\Anaconda3\\envs\\mtech\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'forest', 'space']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "dataset_path = os.listdir('dataset/train')\n",
    "\n",
    "label_types = os.listdir('dataset/train')\n",
    "print (label_types)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa67917c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I collected the videos from pixabay.com "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935f06c4",
   "metadata": {},
   "source": [
    "# Preparing Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7e00542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tag                                        video_name\n",
      "0  car          dataset/train/car/bmw_-_24125 (360p).mp4\n",
      "1  car          dataset/train/car/cars_-_1900 (360p).mp4\n",
      "2  car          dataset/train/car/car_-_11490 (720p).mp4\n",
      "3  car           dataset/train/car/car_-_2165 (360p).mp4\n",
      "4  car  dataset/train/car/mercedes_glk_-_1406 (240p).mp4\n",
      "      tag                                         video_name\n",
      "20  space  dataset/train/space/istockphoto-1399137429-640...\n",
      "21  space  dataset/train/space/istockphoto-539196040-640_...\n",
      "22  space      dataset/train/space/nebula_-_25168 (360p).mp4\n",
      "23  space       dataset/train/space/nebula_-_6044 (360p).mp4\n",
      "24  space  dataset/train/space/rocket_launch_-_236 (240p)...\n"
     ]
    }
   ],
   "source": [
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir('dataset/train' + '/' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str('dataset/train' + '/' +item) + '/' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "train_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(train_df.head())\n",
    "print(train_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a41023c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('car', 'dataset/train/car/bmw_-_24125 (360p).mp4'),\n",
       " ('car', 'dataset/train/car/cars_-_1900 (360p).mp4'),\n",
       " ('car', 'dataset/train/car/car_-_11490 (720p).mp4'),\n",
       " ('car', 'dataset/train/car/car_-_2165 (360p).mp4'),\n",
       " ('car', 'dataset/train/car/mercedes_glk_-_1406 (240p).mp4'),\n",
       " ('car', 'dataset/train/car/new_york_-_26115 (360p).mp4'),\n",
       " ('car', 'dataset/train/car/rallye_-_1295 (240p).mp4'),\n",
       " ('car', 'dataset/train/car/seoul_-_21116 (540p).mp4'),\n",
       " ('car', 'dataset/train/car/traffic_-_243 (360p).mp4'),\n",
       " ('forest', 'dataset/train/forest/161071 (720p).mp4'),\n",
       " ('forest', 'dataset/train/forest/fall_-_28746 (720p).mp4'),\n",
       " ('forest', 'dataset/train/forest/forest_-_111101 (360p).mp4'),\n",
       " ('forest', 'dataset/train/forest/forest_-_17844 (360p).mp4'),\n",
       " ('forest', 'dataset/train/forest/forest_-_97998 (360p).mp4'),\n",
       " ('forest', 'dataset/train/forest/islands_-_2119 (360p).mp4'),\n",
       " ('forest', 'dataset/train/forest/mountains_-_61818 (360p).mp4'),\n",
       " ('forest', 'dataset/train/forest/trees_-_111973 (360p).mp4'),\n",
       " ('space', 'dataset/train/space/abstract_-_2127 (360p).mp4'),\n",
       " ('space', 'dataset/train/space/earth_-_55990 (360p).mp4'),\n",
       " ('space', 'dataset/train/space/fire_-_12910 (360p).mp4'),\n",
       " ('space', 'dataset/train/space/istockphoto-1399137429-640_adpp_is.mp4'),\n",
       " ('space', 'dataset/train/space/istockphoto-539196040-640_adpp_is.mp4'),\n",
       " ('space', 'dataset/train/space/nebula_-_25168 (360p).mp4'),\n",
       " ('space', 'dataset/train/space/nebula_-_6044 (360p).mp4'),\n",
       " ('space', 'dataset/train/space/rocket_launch_-_236 (240p).mp4')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_rooms\n",
    "rooms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27ac79ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset/train/car/bmw_-_24125 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset/train/car/cars_-_1900 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset/train/car/car_-_11490 (720p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset/train/car/car_-_2165 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset/train/car/mercedes_glk_-_1406 (240p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dataset/train/car/new_york_-_26115 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dataset/train/car/rallye_-_1295 (240p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dataset/train/car/seoul_-_21116 (540p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>dataset/train/car/traffic_-_243 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dataset/train/forest/161071 (720p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dataset/train/forest/fall_-_28746 (720p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dataset/train/forest/forest_-_111101 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dataset/train/forest/forest_-_17844 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>dataset/train/forest/forest_-_97998 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dataset/train/forest/islands_-_2119 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dataset/train/forest/mountains_-_61818 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dataset/train/forest/trees_-_111973 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dataset/train/space/abstract_-_2127 (360p).mp4</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dataset/train/space/earth_-_55990 (360p).mp4</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dataset/train/space/fire_-_12910 (360p).mp4</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dataset/train/space/istockphoto-1399137429-640...</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dataset/train/space/istockphoto-539196040-640_...</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>dataset/train/space/nebula_-_25168 (360p).mp4</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dataset/train/space/nebula_-_6044 (360p).mp4</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dataset/train/space/rocket_launch_-_236 (240p)...</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           video_name     tag\n",
       "0            dataset/train/car/bmw_-_24125 (360p).mp4     car\n",
       "1            dataset/train/car/cars_-_1900 (360p).mp4     car\n",
       "2            dataset/train/car/car_-_11490 (720p).mp4     car\n",
       "3             dataset/train/car/car_-_2165 (360p).mp4     car\n",
       "4    dataset/train/car/mercedes_glk_-_1406 (240p).mp4     car\n",
       "5       dataset/train/car/new_york_-_26115 (360p).mp4     car\n",
       "6          dataset/train/car/rallye_-_1295 (240p).mp4     car\n",
       "7          dataset/train/car/seoul_-_21116 (540p).mp4     car\n",
       "8          dataset/train/car/traffic_-_243 (360p).mp4     car\n",
       "9              dataset/train/forest/161071 (720p).mp4  forest\n",
       "10       dataset/train/forest/fall_-_28746 (720p).mp4  forest\n",
       "11    dataset/train/forest/forest_-_111101 (360p).mp4  forest\n",
       "12     dataset/train/forest/forest_-_17844 (360p).mp4  forest\n",
       "13     dataset/train/forest/forest_-_97998 (360p).mp4  forest\n",
       "14     dataset/train/forest/islands_-_2119 (360p).mp4  forest\n",
       "15  dataset/train/forest/mountains_-_61818 (360p).mp4  forest\n",
       "16     dataset/train/forest/trees_-_111973 (360p).mp4  forest\n",
       "17     dataset/train/space/abstract_-_2127 (360p).mp4   space\n",
       "18       dataset/train/space/earth_-_55990 (360p).mp4   space\n",
       "19        dataset/train/space/fire_-_12910 (360p).mp4   space\n",
       "20  dataset/train/space/istockphoto-1399137429-640...   space\n",
       "21  dataset/train/space/istockphoto-539196040-640_...   space\n",
       "22      dataset/train/space/nebula_-_25168 (360p).mp4   space\n",
       "23       dataset/train/space/nebula_-_6044 (360p).mp4   space\n",
       "24  dataset/train/space/rocket_launch_-_236 (240p)...   space"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train_df.loc[:,['video_name','tag']]\n",
    "df.to_csv('train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0897c2",
   "metadata": {},
   "source": [
    "# Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "413b31f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'forest', 'space']\n",
      "Types of activities found:  3\n",
      "      tag                                         video_name\n",
      "0     car         dataset/test/car/street_-_38590 (360p).mp4\n",
      "1     car        dataset/test/car/sunrise_-_22609 (720p).mp4\n",
      "2  forest         dataset/test/forest/fog_-_28690 (720p).mp4\n",
      "3  forest       dataset/test/forest/robin_-_21723 (360p).mp4\n",
      "4   space  dataset/test/space/istockphoto-1411006992-640_...\n",
      "      tag                                         video_name\n",
      "1     car        dataset/test/car/sunrise_-_22609 (720p).mp4\n",
      "2  forest         dataset/test/forest/fog_-_28690 (720p).mp4\n",
      "3  forest       dataset/test/forest/robin_-_21723 (360p).mp4\n",
      "4   space  dataset/test/space/istockphoto-1411006992-640_...\n",
      "5   space       dataset/test/space/nebula_-_24623 (360p).mp4\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.listdir('dataset/test')\n",
    "print(dataset_path)\n",
    "\n",
    "room_types = os.listdir('dataset/test')\n",
    "print(\"Types of activities found: \", len(dataset_path))\n",
    "\n",
    "rooms = []\n",
    "\n",
    "for item in dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir('dataset/test' + '/' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str('dataset/test' + '/' +item) + '/' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "test_df = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(test_df.head())\n",
    "print(test_df.tail())\n",
    "\n",
    "df = test_df.loc[:,['video_name','tag']]\n",
    "df\n",
    "df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45fb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "918d9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_docs.vis import embed\n",
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4fed40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e6c8e4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf44bb",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b716cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos for training: 25\n",
      "Total videos for testing: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>video_name</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>dataset/train/space/istockphoto-539196040-640_...</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>dataset/train/forest/forest_-_111101 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>dataset/train/car/seoul_-_21116 (540p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>dataset/train/forest/forest_-_97998 (360p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>dataset/train/car/new_york_-_26115 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>dataset/train/car/traffic_-_243 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>dataset/train/forest/fall_-_28746 (720p).mp4</td>\n",
       "      <td>forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>dataset/train/car/rallye_-_1295 (240p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>dataset/train/car/car_-_2165 (360p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dataset/train/car/mercedes_glk_-_1406 (240p).mp4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                         video_name     tag\n",
       "21          21  dataset/train/space/istockphoto-539196040-640_...   space\n",
       "11          11    dataset/train/forest/forest_-_111101 (360p).mp4  forest\n",
       "7            7         dataset/train/car/seoul_-_21116 (540p).mp4     car\n",
       "13          13     dataset/train/forest/forest_-_97998 (360p).mp4  forest\n",
       "5            5      dataset/train/car/new_york_-_26115 (360p).mp4     car\n",
       "8            8         dataset/train/car/traffic_-_243 (360p).mp4     car\n",
       "10          10       dataset/train/forest/fall_-_28746 (720p).mp4  forest\n",
       "6            6         dataset/train/car/rallye_-_1295 (240p).mp4     car\n",
       "3            3            dataset/train/car/car_-_2165 (360p).mp4     car\n",
       "4            4   dataset/train/car/mercedes_glk_-_1406 (240p).mp4     car"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(f\"Total videos for training: {len(train_df)}\")\n",
    "print(f\"Total videos for testing: {len(test_df)}\")\n",
    "\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93c182a",
   "metadata": {},
   "source": [
    "# Feed the videos to a network:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcd68b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two methods are taken from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "#  used to crop the center square portion of an image or frame. This can be particularly useful when you want to \n",
    "# focus on a central area of an image or when you want to resize an image to a square aspect ratio while maintaining \n",
    "# the central content.\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "# The load_video function you've provided is designed to load a video from a file path, \n",
    "# preprocess its frames, and return them as a NumPy array. \n",
    "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)\n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[:, :, [2, 1, 0]]\n",
    "#             This line rearranges the color channels of the frame. It switches the order from BGR (Blue-Green-Red) \n",
    "#             to RGB (Red-Green-Blue), which is a common format for image data.\n",
    "            frames.append(frame)\n",
    "\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c243c6ee",
   "metadata": {},
   "source": [
    "   ### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "388c0afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_extractor():\n",
    "    feature_extractor = keras.applications.InceptionV3(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    )\n",
    "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96d6acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_extractor.variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877ca626",
   "metadata": {},
   "source": [
    "### Label Encoding\n",
    "StringLookup layer encode the class labels as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "801339d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['car', 'forest', 'space']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_processor = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "print(label_processor.get_vocabulary())\n",
    "\n",
    "labels = train_df[\"tag\"].values\n",
    "labels = label_processor(labels[..., None]).numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4ac85",
   "metadata": {},
   "source": [
    "Finally, we can put all the pieces together to create our data processing utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18db18a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_data[0].shape)\n",
    "#train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19f38962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['video_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "76216366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define hyperparameters\n",
    "\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "\n",
    "MAX_SEQ_LENGTH = 20\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11befa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame features in train set: (25, 20, 2048)\n",
      "Frame masks in train set: (25, 20)\n",
      "train_labels in train set: (25, 1)\n",
      "test_labels in train set: (6, 1)\n"
     ]
    }
   ],
   "source": [
    "#  data preparation pipeline for video data\n",
    "def prepare_all_videos(df, root_dir):\n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    \n",
    "    ##take all classlabels from train_df column named 'tag' and store in labels\n",
    "    labels = df[\"tag\"].values\n",
    "    \n",
    "    #convert classlabels to label encoding\n",
    "    labels = label_processor(labels[..., None]).numpy()\n",
    "\n",
    "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
    "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
    "    # masked with padding or not.\n",
    "    \n",
    "#     each row in frame_masks corresponds to a video, and each column represents a frame in that video. \n",
    "# If a value in frame_masks is 1, it means that the frame is part of the video, and if it's 0, it means that \n",
    "# the frame is a padding frame.\n",
    "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\") # 145,20\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(os.path.join(root_dir, path))\n",
    "        frames = frames[None, ...]\n",
    "\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :]\n",
    "                )\n",
    "            temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        frame_masks[idx,] = temp_frame_mask.squeeze()\n",
    "\n",
    "    return (frame_features, frame_masks), labels\n",
    "\n",
    "\n",
    "train_data, train_labels = prepare_all_videos(train_df, \"train\")\n",
    "test_data, test_labels = prepare_all_videos(test_df, \"test\")\n",
    "\n",
    "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
    "print(f\"Frame masks in train set: {train_data[1].shape}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"train_labels in train set: {train_labels.shape}\")\n",
    "\n",
    "print(f\"test_labels in train set: {test_labels.shape}\")\n",
    "\n",
    "# MAX_SEQ_LENGTH = 20, NUM_FEATURES = 2048. We have defined this above under hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e0827",
   "metadata": {},
   "source": [
    "# The sequence model\n",
    "Now, we can feed this data to a sequence model consisting of recurrent layers like GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7382f3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0986 - accuracy: 0.5294\n",
      "Epoch 1: val_loss improved from inf to 1.09995, saving model to ./tmp\\video_classifier\n",
      "1/1 [==============================] - 13s 13s/step - loss: 1.0986 - accuracy: 0.5294 - val_loss: 1.0999 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0979 - accuracy: 0.5294\n",
      "Epoch 2: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.0979 - accuracy: 0.5294 - val_loss: 1.1013 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0973 - accuracy: 0.5294\n",
      "Epoch 3: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.0973 - accuracy: 0.5294 - val_loss: 1.1026 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0966 - accuracy: 0.5294\n",
      "Epoch 4: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0966 - accuracy: 0.5294 - val_loss: 1.1040 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0960 - accuracy: 0.5294\n",
      "Epoch 5: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1.0960 - accuracy: 0.5294 - val_loss: 1.1053 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0953 - accuracy: 0.5294\n",
      "Epoch 6: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0953 - accuracy: 0.5294 - val_loss: 1.1066 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0946 - accuracy: 0.5294\n",
      "Epoch 7: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0946 - accuracy: 0.5294 - val_loss: 1.1080 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0940 - accuracy: 0.5294\n",
      "Epoch 8: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 1.0940 - accuracy: 0.5294 - val_loss: 1.1093 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0933 - accuracy: 0.5294\n",
      "Epoch 9: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 1.0933 - accuracy: 0.5294 - val_loss: 1.1106 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0927 - accuracy: 0.5294\n",
      "Epoch 10: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 1.0927 - accuracy: 0.5294 - val_loss: 1.1120 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0920 - accuracy: 0.5294\n",
      "Epoch 11: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0920 - accuracy: 0.5294 - val_loss: 1.1133 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0913 - accuracy: 0.5294\n",
      "Epoch 12: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 1.0913 - accuracy: 0.5294 - val_loss: 1.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0907 - accuracy: 0.5294\n",
      "Epoch 13: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 1.0907 - accuracy: 0.5294 - val_loss: 1.1160 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0900 - accuracy: 0.5294\n",
      "Epoch 14: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1.0900 - accuracy: 0.5294 - val_loss: 1.1173 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0894 - accuracy: 0.5294\n",
      "Epoch 15: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.0894 - accuracy: 0.5294 - val_loss: 1.1187 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0887 - accuracy: 0.5294\n",
      "Epoch 16: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.0887 - accuracy: 0.5294 - val_loss: 1.1200 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0881 - accuracy: 0.5294\n",
      "Epoch 17: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 1.0881 - accuracy: 0.5294 - val_loss: 1.1214 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0874 - accuracy: 0.5294\n",
      "Epoch 18: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1.0874 - accuracy: 0.5294 - val_loss: 1.1227 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0868 - accuracy: 0.5294\n",
      "Epoch 19: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 1.0868 - accuracy: 0.5294 - val_loss: 1.1241 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0861 - accuracy: 0.5294\n",
      "Epoch 20: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 1.0861 - accuracy: 0.5294 - val_loss: 1.1254 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0855 - accuracy: 0.5294\n",
      "Epoch 21: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 1.0855 - accuracy: 0.5294 - val_loss: 1.1268 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0848 - accuracy: 0.5294\n",
      "Epoch 22: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.0848 - accuracy: 0.5294 - val_loss: 1.1281 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0842 - accuracy: 0.5294\n",
      "Epoch 23: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 1.0842 - accuracy: 0.5294 - val_loss: 1.1294 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0835 - accuracy: 0.5294\n",
      "Epoch 24: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 1.0835 - accuracy: 0.5294 - val_loss: 1.1308 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0829 - accuracy: 0.5294\n",
      "Epoch 25: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1.0829 - accuracy: 0.5294 - val_loss: 1.1321 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0823 - accuracy: 0.5294\n",
      "Epoch 26: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 1.0823 - accuracy: 0.5294 - val_loss: 1.1335 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0816 - accuracy: 0.5294\n",
      "Epoch 27: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 1.0816 - accuracy: 0.5294 - val_loss: 1.1348 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0810 - accuracy: 0.5294\n",
      "Epoch 28: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 1.0810 - accuracy: 0.5294 - val_loss: 1.1362 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0803 - accuracy: 0.5294\n",
      "Epoch 29: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1.0803 - accuracy: 0.5294 - val_loss: 1.1375 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/30\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.0797 - accuracy: 0.5294\n",
      "Epoch 30: val_loss did not improve from 1.09995\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 1.0797 - accuracy: 0.5294 - val_loss: 1.1389 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0986 - accuracy: 0.3333\n",
      "Test accuracy: 33.33%\n"
     ]
    }
   ],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "#     1. Input Layers:\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "\n",
    "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
    "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
    "    \n",
    "#     2. GRU Layers: Two GRU (Gated Recurrent Unit) layers are added to the model. \n",
    "#     GRU is a type of recurrent layer used to process sequences of data.\n",
    "    x = keras.layers.GRU(16, return_sequences=True)(frame_features_input, mask=mask_input)\n",
    "    x = keras.layers.GRU(8)(x)\n",
    "    \n",
    "#     3. Dropout Layer:\n",
    "    x = keras.layers.Dropout(0.4)(x)\n",
    "    \n",
    "#     4. Dense Layers:\n",
    "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "\n",
    "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
    "\n",
    "    \n",
    "#     sparse_categorical_crossentropy is commonly used for multiclass classification tasks.\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "\n",
    "# Utility for running experiments.\n",
    "# responsible for running an experiment that trains and evaluates a sequence model for video classification.\n",
    "def run_experiment():\n",
    "    filepath = \"./tmp/video_classifier\"\n",
    "    checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
    "    )\n",
    "#     Purpose of this callback is to save model weights whenever validation performance improves (save the best model).\n",
    "\n",
    "    seq_model = get_sequence_model()\n",
    "    history = seq_model.fit(\n",
    "        [train_data[0], train_data[1]],\n",
    "        train_labels,\n",
    "        validation_split=0.3,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint], # ModelCheckpoint callback should be used during training to save best model weights.\n",
    "    )\n",
    "\n",
    "    \n",
    "#     After training, model weights that resulted in best validation performance are loaded\n",
    "    seq_model.load_weights(filepath)\n",
    "    _, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history, seq_model\n",
    "\n",
    "\n",
    "_, sequence_model = run_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f1681",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "846a5956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test video path: dataset/test/forest/fog_-_28690 (720p).mp4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "  forest: 33.36%\n",
      "  car: 33.36%\n",
      "  space: 33.29%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# is designed to preprocess a single video by extracting frame-level features and creating a mask for the frames.\n",
    "def prepare_single_video(frames):\n",
    "    frames = frames[None, ...]\n",
    "    frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "    frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
    "\n",
    "    for i, batch in enumerate(frames):\n",
    "        video_length = batch.shape[0]\n",
    "        length = min(MAX_SEQ_LENGTH, video_length)\n",
    "        for j in range(length):\n",
    "            frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
    "        frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
    "\n",
    "    return frame_features, frame_mask\n",
    "\n",
    "\n",
    "# designed to perform sequence-based predictions on a single video. \n",
    "# It takes a video file path as input, processes the video frames, and \n",
    "# uses a trained sequence model (sequence_model) to predict the class probabilities for the video.\n",
    "\n",
    "def sequence_prediction(path):\n",
    "    class_vocab = label_processor.get_vocabulary()\n",
    "\n",
    "    frames = load_video(os.path.join(\"test\", path))\n",
    "    frame_features, frame_mask = prepare_single_video(frames)\n",
    "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
    "\n",
    "    for i in np.argsort(probabilities)[::-1]:\n",
    "        print(f\"  {class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
    "    return frames\n",
    "\n",
    "test_video = np.random.choice(test_df[\"video_name\"].values.tolist())\n",
    "print(f\"Test video path: {test_video}\")\n",
    "\n",
    "test_frames = sequence_prediction(test_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6309d87b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
       "        <source src=\"dataset/test/forest/fog_-_28690 (720p).mp4\" type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
       "    </video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "# HTML(\"\"\"\n",
    "#     <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
    "#         <source src=\"dataset/test/forest/robin_-_21723 (360p).mp4\" type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
    "#     </video>\n",
    "# \"\"\")\n",
    "\n",
    "HTML(\"\"\"\n",
    "    <video alt=\"test\" width=\"520\" height=\"440\" controls>\n",
    "        <source src=\"dataset/test/forest/fog_-_28690 (720p).mp4\" type=\"video/mp4\" style=\"height:300px;width:300px\">\n",
    "    </video>\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6150d7e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b16e337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15f322",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
